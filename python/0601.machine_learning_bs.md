# Machine Learning

- 머신러닝 (기계학습)
    - 인공지능 한 분야
    - 데이터를 이용하여 명시적으로 정의되지 않은 패턴을 학습해서 미래 결과(값, 분포)를 예측
    - 샘플 데이터인 Training Data를 사용하여 수학적 모델을 생성하고 생성된 모델을 예측 또는 의사결정에 활용

    - 특징
        - 복잡한 문제를 데이터를 기반으로 숨겨진 패턴을 인지해서 해결
        - 다양한 수학적 기법 사용
        - 머신러닝 알고리즘은 데이터를 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화
        - 데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과 도출


## 분류 모델 
: 회귀 / 분류 중 분류를 이용하는 지도학습 방법

<br>
    
- 지도학습(Supervised Learning)
    - 학습할 데이터의 입력과 그 입력에 대응하는 정답을 이용해서 데이터의 특성과 분포를 학습하고 미래 결과를 예측하는 방법
    - 새로운 입력값에 대한 예측 수행
        - 회귀 (출력값이 수치형)
        - 분류 (출력값이 범주형)


### 1. 데이터 준비


#### scikit-learn (sklearn)
  : python 기반의 기계 학습 분야에서 주로 활용되는 오픈소스 라이브러리

- 함수 및 클래스

    - load_iris() : 붓꽃 데이터 세트
    - DecisionTreeClassifier 클래스 : ML 알고리즘에서 사용하는 의사결정 트리 알고리즘
        - 의사결정 트리 분류 모듈 DecisionTreeClassifier 
    - train_test_split() : 데이터 세트를 학습 데이터와 테스트 데이터 세트로 분리


```py
!pip install scikit-learn

import sklearn
print(sklearn.__version__)  # 버전 확인
```

- 붓꽃 데이터 로딩
```py
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# 데이터 프레임으로 변환

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)


```

### 2. 데이터 세트 분리 : 학습데이터 / 테스트 데이터 세트

- train_test_split()
    - train_test_split(피처 데이터 세트, 레이블 데이터 세트, 테스트 데이터 세트 비율, 난수 발생값)
    - ex) train_test_split(iris_data, iris_label, test_size=0.3, random_state=11)
        - random_state
        <br>
          : 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값을 동일하게 유지하기 위해 random_state 설정

- train_test_split() 반환값 : X_train, X_test, y_train, y_test  
    - X_train : 학습용 피처 데이터 세트 (feature)
    - X_test : 테스트용 피처 데이터 세트 (feature)
    - y_train : 학습용 레이블 데이터 세트 (target= 예측값) 
    - y_test : 테스트용 레이블 데이터 세트 (target= 예측값) 
    - feature : 대문자 X_
    - label(target) : 소문자 y_

```py
X_train, X_test, y_train, y_test = train_test_split(iris.data, 
                                                    iris.target, # iris.target은 예측해야 할 값
                                                    test_size=0.2,  # 테스트 세트로 나눌 비율
                                                    random_state=11) 
```

### 3. 모델 학습 : 학습 데이터 세트 기반으로 ML 알고리즘을 적용

- ML 알고리즘으로 의사결정 트리 알고리즘을 이용해서 학습과 예측 수행  
- DecisionTreeClassifier 클래스의 fit()/predict() 메소드 사용  

- fit() : 학습 수행 (학습용 데이터 사용)  
    - fit(학습용 피처 데이터 세트, 학습용 레이블(정답) 데이터 세트)

- predict() : 예측 수행  
    - predict(테스트용 피처 데이터 세트)


```py
# DecisionTreeClassifier 객체 생성
dt_clf = DecisionTreeClassifier(random_state=11)  # 결정 트리 분류기는 노드에서 불순도가 가장 크게 감소하는 결정 규칙을 찾는다. 

# 학습용 데이터 세트로 학습 수행 : 피처와 레이블(정답) 학습
# fit(학습용 피처 데이터, 학습용 레이블 데이터(정답)) 메소드 사용
dt_clf.fit(X_train, y_train)

# 예측된 데이터 세트 반환
y_pred = dt_clf.predict(X_test)

# 실제 데이터 레이블 값과 예측한 값 비교
print(y_pred)
print(y_test)

# 비교 결과
# 30개 중 2개만 예측이 빗나갔고, 28개 정확하게 예측
# 28/30 = 0.9333
```

### 4. 평가: 예측 정확도 평가
    - 예측된 결과값과 테스트 데이터의 실제 결과값과 비교해서 ML 모델 성능 평가
    - accuracy_score(실제 레이블(정답) 값, 예측한 값) 사용
<br>
      (accuracy: 정합도, 모델이 정답을 정답으로, 오답을 오답으로 선택하는가를 보여주는 지표)

```py
from sklearn.metrics import accuracy_score

print('예측 정확도 : {0:.4f}'.format(accuracy_score(y_test, y_pred)))  # {0:.4f}.format은 보여줄 소수점의 자리수 결정을 위한 것 뿐이니 크게 신경쓰지 말 것! 

```

### 5. 교차 검증 (Cross Validation)
: 머신러닝/딥러닝에서 데이터를 통한 모델을 설계한 후 모델을 검증하기 위한 단계.
- Traim : test = 7 : 3 or 8 : 2로 검증
- 여기서 더 좋은 모델을 만들기 위해 다시 train data를 7:3으로 나눠, 검증함. = 과적합 방지

<br>

#### method
- KFold(n_splits = n)
<br>
  : n개의 폴드 세트로 분리하는 함수

```py
KFold(n_splits = 10, shuffle = True, random_state =1)  # n_splits는 폴드 개수, random_state은 seed와 같은 역할
```

- cross_val_score()
    1. 폴드세트 설정
    2. for 루프에서 반복으로 학습 및 테스트 데이터의 인덱스를 추출
    3. 반복적으로 학습과 예측을 수행하고 예측 성능을 반환

```py
cross_val_score(pipeline,  # 의사결정나무 모델
                X_train,  # features
                y_train,   # targets
                cv=5, # 교차검증 회수
                scoring="accuracy")  # 평가지표
```

- GridSearchCV()
: 위의 두 개를 한 번에 실행? 한다는 느낌! 

```py
parameters = {
    'max_depth' : [1,3,5],
    'min_samples_split' : [2, 3, 5], 
    'min_samples_leaf' : [1, 5, 8]
}  # parameters 부분은 내가 조정!

grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)
grid_dclf.fit(X_train, y_train)

print('GridSearchCV 최적 하이퍼 파라미터 : ', grid_dclf.best_params_)
print('GridSearchCV 최고 정확도 : {0:.4f} : '.format(grid_dclf.best_score_))

best_dclf = grid_dclf.best_estimator_

y_pred = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print('하이퍼 파라미터 튜닝을 끝낸 최종 정확도 : {0:.4f} : '.format(accuracy))


# DF로 내용 전부 출력

result_df = pd.DataFrame(grid_dclf.cv_results_['params'])
result_df['mean_test_score'] = grid_dclf.cv_results_['mean_test_score']
result_df.sort_values(by = 'mean_test_score', ascending= False)
```

## 추가적인 내용

- 범주형 데이터 인코딩

1. 자동 인코딩
```py
# 인코딩 : 사이킷런의 LabelEncoder 클래스 이용해서 레이블 인코딩 적용

from sklearn import preprocessing

# 레이블 인코딩 함수 작성
def encode_features(df_data) :
    features = ['Cabin', 'Sex', 'Embarked']   # 인코딩할 열 선택
    
    # 각 피처에 대해 반복 수행
    for feature in features :
        le = preprocessing.LabelEncoder() # LabelEncoder 객체 생성
        le = le.fit(df_data[feature])
        df_data[feature] = le.transform(df_data[feature])
        
    return df_data

titanic_df = encode_features(titanic_df)
titanic_df.head()    
```

2. 수동 인코딩 (map 사용)
```py
# map() 사용하여 피처 값 수치 변환
titanic_df2 =  pd.read_csv('data/titanic_train.csv')

embarked_map = {
    'S' : 0,
    'C' : 1,
    'Q' : 2,
    'N' : 3
}

titanic_df2['Embarked'] = titanic_df2['Embarked'].map(embarked_map)
titanic_df2.head(5)
```