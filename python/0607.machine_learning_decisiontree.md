# machine_learning

## 1. decision tree (의사결정나무)
![decision_tree](https://heung-bae-lee.github.io/image/consist_of_decision_tree_node_01.png)

: 자료를 학습하여 특정 분리 규칙을 찾아내고, 그에 따라 소집단으로 분류하는 분석 방법

<br>

- Root Node : 
    - 모든 데이터를 포함하는 의사결정 트리의 줄발점
    - 데이터를 몇 개의 동일한 그룹으로 분할
- Leaf Node
    - 최종 결과를 나타내는 노드
    - 데이터를 더 이상 분할하지 않음
- Parent/Child Node
    - 상위 노드 / 하위 노드
- Spliting
    - 주어진 조건에 따라 데이터를 분할하는 작업
- Branch/Sub Tree
    - 트리를 분리하여 생성된 하위 트리
- Depth 
    - Root부터 마지막 노드까지 경로의 길이 

<br>

- 특징 
    - 이산형 / 연속형 변수 모두에 적용이 가능함

<br>

- 단점
    1. 독립변수들 간의 중요도 차이 판단이 모호함
    2. 분류 경계선 근처의 자료에 대한 오차가 많음
    3. 규칙이 많아짐 -> 결정 방식이 복잡해짐 = depth가 길어짐 -> 과적합 -> 예측 성능의 저하 가능성

<br>


- 분리기준
    1. 불순도: 자료들의 범주가 한 그룹 안에 얼마나 섞여 있는지
    2. 종속변수가 범주형이면 분류트리 사용  => (끝 마디에 포함된 자료의 범주가 분류 결과 값)  = 판별 = Classification
    3. 종속변수가 연속형이면 회귀트리 사용  => (끝 마디에 포함된 자료의 평균값이 각 끝마디의 회귀값)  = 예측 = Regression Tree != Logistic Regression

        * 회귀트리는 회귀분석에 비해 성능이 떨어지기 때문에, 차라리 회귀분석을 사용하자!

<br>

- 균일도
    : 같은 것 끼리 섞여있는 정도
    - 여러가지가 섞여 있는 경우 균일도가 낮고 혼잡도가 높음
    - 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만드는 것이 중요함.

<br>

- method


<br>

- **분류트리** 결정 과정
    1. 정보 이득이 높거나 지니 계수가 낮은 조건을 찾음
<br>
    (데이터 집합의 모든 아이템이 같은 분류에 속하는지 확인)

<br>
    2. 자식 트리 노드에 걸쳐 반복적으로 분할
<br>
    (If True: 리프 노드로 만들어서 분류
    Else: 데이터를 분할하는 데 가장 좋은 속성과 분할 기준을 찾음 = 정보이득 / 지니계수 이용)

        - 지니계수
          : 불순도를 수치화한 지표로, 0이 가장 평등, 1로 갈수록 불평등

<br>
    3. 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류 결정

<br>
    (해당 속성과 분할 기준으로 데이터를 분할하여 Branch 노드 생성 -> 모든 데이터 집합의 분류가 결정될 때까지 수행)

<br>

### *분류트리 사용*

<br>

1. 데이터 세트 로드 (iris)
```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 데이터 세트 로드
iris_data = load_iris()

# 학습/테스트 데이터 세트로 분리
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, 
                                                    iris_data.target, 
                                                    test_size=0.2, 
                                                    random_state=11)

# X_train : 학습용 피처 데이터 세트 (feature)
# X_test : 테스트용 피처 데이터 세트 (feature)
# y_train : 학습용 레이블 데이터 세트 (target= 예측값)
# y_test : 테스트용 레이블 데이터 세트 (target= 예측값)
```
<br>

2. 의사결정 트리 생성
```py
from sklearn.tree import DecisionTreeClassifier

# DecisionTreeClassifier 생성
dt_clf = DecisionTreeClassifier(random_state = 156)  # random_state = seed와 같은 고정하기 위한 값

# random_state 동일하게 트리 생성
dt_clf.fit(X_train, y_train)  # fit 함수는 머신러닝을 학습하라는 명령어
```

<br>

3. 학습데이터와 테스트 데이터 세트 확인
```py
# 피처 이름과 타겟 이름 확인
print(iris_data.feature_names)  # 속성값
print(iris_data.target_names)  # 예측값
```

<br>

4. 트리 그래프 출력
```py
# plot_tree() 사용하여 트리 그래프 출력
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10, 7))  # figsize는 그래프 크기 조정용인거 잊지 말기!
plot_tree(dt_clf, max_depth=2, filled=True, feature_names=iris_data.feature_names)
plt.show()

```

- plot_tree method
    - min_samples_split : 노드를 분할하기 위한 최소'샘플' 수. 작을수록 과적합 가능성 증가 | 높을 수록 과적합 방지, but 정확도와 예측력이 낮아짐

    - min_samples_leaf : 말단 노드가 되기 위한 최소'샘플' 수. 비대칭적 데이터의 경우 작게 설정 필요

    - max_features : 최적분할을 위해 고려할 최대 피처 개수.

        - int: 피처개수, float: 피처 퍼센티지

        - auto = sqrt = 전체피처 수의 제곱근개

        - log = 전체 피처 중 log2(전체 피처 개수) 선정

        - None: 전체피처 선정

    - max_depth : 트리 최대 깊이 규정. None일 시 완벽한 클래스 결정값이 될 때까지 깊이를 증가시킴

    - max_leaf_nodes : 말단 노드의 최대 개수

    - criterion : 의사결정나무는 정보의 균일도 측도에 따라 데이터를 분류한다.

    - 정보의 균일도 측도

        1) 정보이득지수('entropy') :

        - 1-엔트로피. 높은 값을 기준으로 분할 기준 결정

        2) 지니계수('gini') - 디폴트값

        - 0으로 갈수록 평등하고, 1로 갈수록 불평등함. 낮은 값을 기준으로 분할 기준 결정

<br>

5. 나온 트리 그래프로 필터링해서 조건에 맞는 자료 확인
```py
# 위의 트리 그래프를 기준으로하는 필터링

df = iris_data_df[(iris_data_df['petal length (cm)'] > 2.45) & \
                        (iris_data_df['petal width (cm)'] > 1.55) & \
                        (iris_data_df['petal width (cm)'] > 1.75)]

df['target'].value_counts()
```

<br>

6. 하이퍼 파라미터 튜닝

- 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면, 트리 노드를 계속해서 만들어낸다
   -> 모델이 쉽게 과적합되는 문제 발생
   -> 하이퍼 파라미터를 사용하여 복잡한 트리가 생성되지 않도록 제어

- 기본적으로는 parameters = { 이 안에 plot_tree method를 이용하여 파라미터를 설정 }

- GridSearchCV 클래스
    : 가능한 모든 조합을 작동하여 가장 적합한(최적의) 하이퍼 파라미터를 결정

#### GridSearchCV()

- parameter

    - param_grid: 하이퍼파라미터의 이름의 문자열을 key로 갖고, 시도하고자 하는 하이퍼파라미터의 값들의 list를 대응되는 value로 갖는 dictionary를 전달.
    - scoring: 각 모델을 평가하는 기준인 하나의 또는 여러 개의 scoring 방법을 지정
    - n_jobs: 병렬적으로(in parallel) 수행되는 작업의 개수를 지정
    - refit: 최적의 하이퍼파라미터 조합을 확인한 후, 전체 데이터셋에 대하여 이 조합의 모델을 훈련시킬지 여부를 결정
    -  cv: 교차검증의 구체적인 방법론을 결정
        -  int 값을 입력 =  k-fold 교차검증방법의 폴드 수를 의미

    - pre_dispatch: 병렬적으로 작업이 이루어질 때 전송되는 작업의 수를 결정. 이 값을 줄여 CPU가 감당할 수 있는 양 이상의 계산량이 할당되는 것을 방지

- attributes
    - cv_results_ : 각 하이퍼파라미터에서 고려되는 값들, fitting과 scoring에 소요된 시간의 평균과 표준편차, 각 하이퍼파라미터 조합과 폴드에서의 score의 평균과 표준편차가 정리된 dictionary


1 
```py
dt_clf = DecisionTreeClassifier(min_samples_leaf=4, random_state=156)  # 여기서 튜닝

# 학습
dt_clf.fit(X_train, y_train)

# 그래프 출력
plt.figure(figsize=(10, 7))
plot_tree(dt_clf, filled=True, feature_names=iris_data.feature_names)
plt.show()
```

<br>

2
```py
# 하이퍼 파라미터 설정
parameters = {
    'max_depth': [3,4,5],
    'min_sample_split':[1, 2,3],
    'min_sample_leaf':[2,3,4]
}

# GridSearchCV 이용해서 최적의 하이퍼 파라미터 찾음

grid_dclf = GridSearchCV(rf_clf, param_grid=parameters, scoring='accuracy', cv=2)
grid_dclf.fit(X_train, y_train)

print('GridSearchCV 최적 하이퍼 파라미터 : ', grid_dclf.best_params_)
print('GridSearchCV 최고 정확도 : {0:.4f} : '.format(grid_dclf.best_score_))


# 최적 하이퍼 파라미터로 학습된 Estimator로 예측/평가 수행

best_dclf = grid_dclf.best_estimator_

y_pred = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print('하이퍼 파라미터 튜닝을 끝낸 최종 정확도 : {0:.4f} : '.format(accuracy))

```

<br>

7. feature_importance_
    : tree를 분할할 때 각 속성이 얼마나 중요한지 평가하는 수치
<br>
      (0~1 사이의 숫자)

     - ndarray 형태로 값을 반환
     - 피처 순서대로 값이 할당됨 
     - 특정 노드의 중요도 값이 클수록, 그 노드에서 불순도 역시 감소함

```py
# 위 예제에서 fit() 학습된 DecisionTreeClassifier 객체 df_clf의 feature_importance_ 속성 사용
import seaborn as sns
import numpy as np
%matplotlib inline

# 피처 이름
print('iris_data.feature_names : ', iris_data.feature_names)

# 피처별 중요도 값(feature importance) 출력
print('Feature importances:\n{0}'.format(np.round(dt_clf.feature_importances_ , 3)))

# 동일한 개수로 이루어진 데이터를 묶어주는 함수
for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):
    print('{0}: {1:.3f}'.format(name, value))    
```

<br>

## 2. 로지스틱 회귀분석

<br>
    : 독립변수의 선형 결합을 이용하해 사건의 발생 가능성을 예측하는 방법

    + 회귀트리와 회귀분석
    : 분류트리가 지니계수나 엔트로피가 최소화 되는 분할 기준을 찾을 때 까지 모두 분할했듯, 회귀트리는 평균의 제곱 오차가 가장 적어지도록 하는 변수를 찾아서 분할한다. 
        -> 변수의 출력값들의 평균으로부터 제곱 오차가 가장 적게 분류 함
        = 분류된 변수들 끼리 평균값이 가장 가깝도록 분류  = 평균값을 통해 예측값 도출

          =>  차라리 회귀분석을 하는게 기능적으로 낫다!

<br>

    - 독립변수 = 연속형 (범주형일시, 더미변수로 변환하여 사용)
    
    - 종속변수 = 범주형 일때 사용
        -> 종속변수는 항상 0과 1 사이의 값을 가짐 = 오즈, 로짓변환, 시그모이드 함수로 확률값을 예측함.
        -> 예측을 성공 or 실패 로 반환

    - 선형회귀와 달리 시그모이드 함수의 최적선을 찾고, 함수의 반환값을 확률로 간주하며, 확률에 따라 분류를 결정한다 
    (시그모이드 함수는 로짓의 역함수 관계)  

<br>

### LogisticRegression()

- parameter
    - penalty: {'l1', 'l2', 'elasticnet', 'none'} 을 선택해서 사용. 기본값은 'l2'
        - 규제에 관해서
          : 규제는 과대적합을 해결하기 위한 하나의 방법이다 
            (과대적합해결 방법: 1.더 많은 훈련데이터를 모으기  2.파라미터 개수가 적은 간단한 모델을 선택  3.데이터 차원 줄이기)

            - L2 : 개별 가중치 값을 제한하여 모델 복잡도를 줄이는 방법
                - 비용 함수에 페널티 항을 추가하여 규제가 없는 비용 함수로 훈련한 모델에 비해 가중치 값을 아주 작게 만드는 효과를 만든다
                - 모델을 학습할 만한 충분한 훈련데이터가 없을 때 편향을 추가하여 모델을 간단하게 만드는 것으로 분산을 줄인다
                   (규제 강도가 커질수록 추정된 계수의 절댓값이 작아짐)

            - L1 : L2와 비슷하지만, 가중치 절댓값의 합을 제한한다
                    ( 필요없는 특성변수의 계수를 아예 0으로 보내버림)

    - l1_ratio: penalty = elasticent 일 때, l1 규제 비율을 정하는데 사용함. (0~1의 실수값 사용)

    - C : 규제의 강도를 조절. C값이 클 수록 규제가 약해지고, 작을수록 규제가 강해진다 = 값이 클수록 단순한 모델이 됨
    
    - class_weight: 학습 시 클래스에 따라 가중치를 다르게 주고 싶을 때 사용할 수 있는 파라미터
        - 딕셔너리 형태로 직접 적용 가능 {클래스: 가중치값}
        - balaced 옵션 = 전체 샘플 수 / (클래스 수 * 클래스별 빈도) => 불균형 데이터셋을 다루는 상황에 사용

    - solver: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'} 중에 선택해서 사용
        (최적화 문제를 풀기 위한 알고리즘을 선택하는 파라미터)

    ![solver](https://i.stack.imgur.com/wSLpz.png)


    - random_state: random하게 뽑히는 데이터를 고정하는 역할

    - max_iter: solver에 의해 진행되는 수렴을 위한 반복의 최대 횟수를 지정

    - multi_class: 다중클래스분류 문제의 상황에서의 접근 방식을 결정함.
        {'ovr', 'multionmial', default = 'auto'} 
            - ovr : 각 클래스 레이블에 대해 이진 분류 문제를 품
            - multionmial: 클래스에 대한 softmax 확률값, 크로스 엔트로피 계산을 통해 타겟 벡터를 생성하는 식으로 다중분류를 수행

    - n_job: 모델 running시 동시에 사용할 CPU 코어 수를 지정

<br>

- attributes

    - classes_  : 모형에 알려진 클래스 레이블 목록을 반환
    - coef_ : 피팅된 모형의 특성 가중치 추정값을 array 형태로 반환
             ( 이진분류시: (1, 특성) 형태
               다중분류일 경우: (클래스 수, 특성 수) 형태 )
    - intercept_ : 피팅된 모형의 절편 추정값을 array 형태로 반환
             ( 이진분류: 단일 절편  |  다중분류: 각 클래스에 대응되는 절편 추정 값)


<br>

- methods
    - fit(x,y) : x,y 훈련 셋을 입력하여 모형을 fitting한다 = 훈련시킨다
    - predict(x) : x_testset 을 입력하여 각 관측에 대해 클래스 레이블 예측값을 array 형태로 반환
    - score(x,y) : x_tesrset, y_tesrset 을 입력하여 모형의 예측 정확도(accuracy) 평가
    - decision_function(x) : x_testset 을 입력하여 각 클래스 별로 어느 정도의 레이블 확신 점수를 갖는지 반환
                            (확신 점수는 관측이 각 클래스의 분류 초평면으로부터 떨어진 거리를 기반으로 계산)
    - predict_proba(x) : predict의 결과로 나올 확률값(0~1)


<br>


1. 모델 사용 기본
```py
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  # 모델의 생성
model.fit(features, labels)  # 모델 학습시키기  / fit() 메서드는 모델에 계수(model.coef)와 절편(model.intercept)이라는 두 변수를 전달한다

model.predict(new_features)  # 새로운 속성을 넣어 예측해달라고 명령 -> 그 레이블에 속하는지 아닌지를 1 혹은 0으로 구성된 벡터를 반환하여 대답함

model.predict_proba(new_features)  # 해당 레이블로 분류될 확률값을 알고 싶을 때 명령. (0~1 사이의 값으로 대답)

```


2. 데이터 불러오기 & 전처리
```py
import pandas as pd
data = pd.read_csv("passengers.csv")
print(data.shape)  # 데이터의 속성 즉, 데이터의 개수와 칼럼(열)의 개수 및 종류를 알 수 있음
print(data.head())

# 전처리
# 사용할 열을 고르고, 범주형 변수라면 더미변수로 고칠 것
# 결측치 채우고, feature 분리를 위한 새로운 칼럼이 필요하다면 생성하기

```

3. 데이터 세트 분리

```py
X_train , X_test , y_train , y_test = train_test_split(data, 
                                                       data.target, 
                                                       test_size=0.3, 
                                                       random_state=0)
```

4. 데이터 스케일링(정규화)

```py
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()  # 스케일할 도구로 sd를 고름 
                           # 1) 최소값 0, 최대값1로 변환하는 MinMaxScaler()
                           # 2) 중앙값(median) 0, IQR(interquartile range) 1이 되도록 변환하는 RobustScaler() 등이 있음

train_features = scaler.fit_transform(train_features)  # fit과 transform을 합친 것 
                                                       # fit: 각 속성마다 컬럼을 만드는 역할
                                                       # transform: 데이터를 변형시킴
                                                       # fit_transform은 train 데이터셋에만 사용된다. 

# fit_transform
# : 우리가 만든 모델은 train data에 있는 mean과 variance를 학습하게 된다. 
#   -> 이렇게 학습된 Scaler()의 파라미터는 test data를 정규화 하는데 사용

#   => train data로 학습된 Scaler의 파라미터를 통해 test data의 속성값들이 정규화됨

test_features = scaler.transform(test_features)  # transform: train data로부터 학습된 mean값과 variance값을 test data에 적용하기 위해 사용

```
- test data에서 fit_transform을 사용하지 않는 이유
  : 만약, fit_transform을 test data에도 적용하면, test data로부터 새로운 평균과 분산을 얻게 됨
    = 예측 모델이 test data의 데이터 값도 학습하게 됨 
    = 예측이 아니라 학습한 것에서 값을 찾는 꼴이 되겠죠! = 모델 성능 평가 불가!


5. 모델 생성 및 평가하기

```py
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  # parameter 사용하는 부분
model.fit(train_features, train_labels)

print(model.score(train_features, train_labels))  # train 데이터로 정확도 확인하기
print(model.score(test_features, test_labels))  # test 데이터로 예측 정확도 확인하기
```

6. 예측 및 정확도 출력
```py
y_pred = lr_clf.predict(X_test)
print('accuracy : {0:.4f}'.format(accuracy_score(y_test, y_pred)))  # 정확도 출력
```


7. 완전히 새로운 값을 넣어 예측해보기

```py
sample_newwww_data = scaler.transform(sample_newwww_data)  # 스케일링

print(model.predict(sample_newwww_data))  # 예측 (0,1)
print(model.predict_proba(sample_newwww_data))  # 예측에 대한 확률

```

8. 하이퍼 파라미터 튜닝

```py
from sklearn.model_selection import GridSearchCV

# 하이퍼 파라미터 설정
parameters = {
    'max_iter' : [100, 500, 1000],
    'penalty' : ['l2', 'l1'],
    'C' : [0.01, 0.1, 1, 5, 10]
}

# GridSearchCV 이용해서 최적의 하이퍼 파라미터 찾음
hyper_model = GridSearchCV(model, param_grid=parameters, scoring='accuracy', cv=5)
hyper_model.fit(X_train, y_train)

print('GridSearchCV 최적 하이퍼 파라미터 : ', hyper_model.best_params_)
print('GridSearchCV 최고 정확도 : {0:.4f} : '.format(hyper_model.best_score_))

# 최적 하이퍼 파라미터로 학습된 Estimator로 예측/평가 수행
best_dclf = hyper_model.best_estimator_
y_pred = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print('하이퍼 파라미터 튜닝을 끝낸 최종 정확도 : {0:.4f} : '.format(accuracy))
```



<br>

## 2. 최근접 알고리즘 (KNN)
    : K개의 가까운 이웃의 속성에 따라 분류

- 정답 라벨이 있는 데이터 속 정답 라벨이 없는 데이터 분류에 사용됨
- 새롭게 입력받은 데이터로부터 가장 가까이 있는 데이터의 정답라벨을 확인
<br>

  -> 새로운 데이터의 정답라벨 결정
  => 최적의 정답라벨을 찾을 수 있는가가 관건

<br>

1. 레이블 데이터 생성
```py
fish_target = np.concatenate((np.ones(35), np.zeros(14)))  # 속성이 2개인데, 각각 35개/14개의 데이터가 있어서 레이블을 이렇게 만들었어요~
```
<br>

2. 데이터 세트 분리 (학습/ 테스트 데이터 세트 만들기)
```py
X_train, X_test, y_train, y_test = train_test_split(fish_data, 
                                                    fish_target, 
                                                    stratify = fish_target,
                                                    random_state=42)
```

<br>

3. 모델 학습
```py
from sklearn.neighbors import KNeighborsClassifier

kn_clf = KNeighborsClassifier()
kn_clf.fit(X_train, y_train)

```

<br>

4. 모델 평가
```py
kn_clf.score(X_test, y_test) # 1.0이 나오면 100% 
```

<br>

5. 모델 예측
```py
print(kn_clf.predict([[25, 150]]))  # 25, 150의 값을 넣은 결과가 위에서 만든 레이블의 속성 중 1로 예측해야 하는데, 0으로 예측하는 결과가 나옴

# 산점도 확인
plt.scatter(X_train[:,0], X_train[:,1])  # 산점도 그리는 방법 다시 확인 필요...
plt.scatter(25,150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 특정 값의 이웃한 값들 확인하기

dist, idx = kn_clf.kneighbors([[25, 150]])   # n_neighbors의 기본값이 5
print(dist)
print(idx)

# 길이와 무게가 1대1로 모델에 반영되려면 크기가 비슷해야 하는데, 길이에 비해 무게가 크기가 너무 커서 길이와 무게가 다른 비율로 반영됨

```

<br>

6. 스케일링
    : 모델에 반영될 두 속성의 크기가 너무 다를 때 다른 비율로 반영되어 나타나는 문제점을 해결하기 위한 방법

    - 방법1: 최소-최대 정규화: 변수 x의 범위를 0~100%로 표현
    - 방법2: z-점수 표준화 (원점수 - 평균) / 표준편차

    - ** 스케일링을 진행했다면, 테스트 값도 스케일링 해야됨!! **

```py

X_train_scaled = (X_train - mean)/std
X_test_scaled = (X_test - mean) /std

# 스케일된 데이터로 그래프 출력
plt.scatter(X_train_scaled[:,0], X_train_scaled[:,1])

# x축과 y축의 범위가 같아져 분포가 동일함을 확인

# 테스트 값 스케일링
test_scaled = ([25, 150] - mean) / std
X_test_scaled = (X_test - mean) /std


# 학습 : 스케일링된 학습 데이터로 재학습
kn_clf.fit(X_train_scaled, y_train)


# (25, 150)을 스케일된 수치로 예측 : test_scaled
kn_clf.predict([test_scaled]) 

# 정확도 출력
kn_clf.score(X_test_scaled, y_test) # 1.0

# 이후에 위와 같이 산점도를 출력하여서 다시 확인하기!

```

##



## 추가적인 내용

### zip 함수

```py
# zip() 함수를 사용해서 튜플로 딕셔너리 생성
fruits = ('apple', 'pear', 'peache')
prices = (100, 250, 150)

for key, value in zip(fruits, prices) :
    print(key, value)
```

### 데이터 합치기
- np.concatenate 
  : 1차원 배열로 합치기 (옆으로)

- np.column_stack
  : 2차원 배열로 합치기

- np.full()

```py
import numpy as np
a = [10, 20, 30]
b = [1, 2, 3]

np.concatenate((a,b))
np.column_stack((a,b))  #  2치원 형태로 합치기

# [] array([10, 20, 30,  1,  2,  3])
# [] array([[10,  1],
#           [20,  2],
#           [30,  3]])


np.full(3, 1) # 1차원 : 1로 생성
np.full(4, 0) # 0으로 생성
np.full((3, 4), 0) # 2차원 배열 형태

# [] array([1, 1, 1])
# [] array([0, 0, 0, 0])
# [] array([[0, 0, 0, 0],
#           [0, 0, 0, 0],
#           [0, 0, 0, 0]])

```

### numpy의 array 생성함수

- np.ones(shape, dtype, order)
    : 1로 가득찬 array를 생성

- np.zeros(shape, dtype, order)
    : 0으로 가득찬 array를 생성

- empty()
    : empty는 초기화되지 않은 값으로 zeros나 ones와 마찬가지로 배열을 생성.
    (주의: 메모리도 초기화되지 않기 때문에 예상하지 못한 불필요한 값이 들어갈 수 있음)
